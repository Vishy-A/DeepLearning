{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPzZP6k9GiNf6U8eG1Ce4L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vishy-A/DeepLearning/blob/main/HW5/4106HW5P2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "01vFQKFLUYGl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import requests\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "response = requests.get(url)\n",
        "seq = response.text\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e55oKMspVWYX",
        "outputId": "030857bf-0833-4890-e897-171255c06afc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(seq)))\n",
        "char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
        "int_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "encoded_text = [char_to_int[ch] for ch in seq]"
      ],
      "metadata": {
        "id": "hAlJRu6-Vwqz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def definition(maxlen):\n",
        "    x = []\n",
        "    y = []\n",
        "    for i in range(len(seq) - maxlen):\n",
        "        sequence = seq[i:i + maxlen]\n",
        "        label = seq[i + maxlen]\n",
        "        x.append([char_to_int[char] for char in sequence])\n",
        "        y.append(char_to_int[label])\n",
        "\n",
        "    x = np.array(x)\n",
        "    y = np.array(y)\n",
        "\n",
        "    return x, y\n"
      ],
      "metadata": {
        "id": "_XFgOdGZVylu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CharDataset(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.x[idx], self.y[idx]"
      ],
      "metadata": {
        "id": "7j2EklKJV2sc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "x, y = definition(20)\n",
        "x = torch.tensor(x, dtype=torch.long)\n",
        "y = torch.tensor(y, dtype=torch.long)\n",
        "dataset20 = CharDataset(x, y)\n",
        "train_size = int(len(dataset20) * .8)\n",
        "val_size = len(dataset20) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset20, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "7knPI9fdV-u3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers, num_heads, dropout):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        encoderlayers = nn.TransformerEncoderLayer(hidden_size, num_heads, dropout=dropout)\n",
        "        self.transformer = nn.TransformerEncoder(encoderlayers, num_layers)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output = self.transformer(embedded)\n",
        "        output = self.fc(output[:, -1, :])\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "gcMLlb_TWB_y"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(train_loader, val_loader, model, lossfunc, optimizer, epochs):\n",
        "    model.to(device)\n",
        "    val_accs = []\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = 0\n",
        "        val_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        model.train()\n",
        "\n",
        "        for x, y in train_loader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(x)\n",
        "            loss = lossfunc(output, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * x.size(0)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for x, y in val_loader:\n",
        "                x = x.to(device)\n",
        "                y = y.to(device)\n",
        "                output = model(x)\n",
        "                loss = lossfunc(output, y)\n",
        "                val_loss += loss.item() * x.size(0)\n",
        "                _, predicted = torch.max(output.data, 1)\n",
        "                total += y.size(0)\n",
        "                correct += (predicted == y).sum().item()\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "        val_loss /= len(val_loader)\n",
        "        acc = correct / total\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accs.append(acc)\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Validation Accuracy: {acc:.2f}%')\n",
        "\n",
        "    return train_losses, val_losses"
      ],
      "metadata": {
        "id": "YR5ARhjbWE2K"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 128\n",
        "num_layers = 3\n",
        "num_heads = 2\n",
        "dropout = 0.1\n",
        "lr = 0.001\n",
        "epochs = 10\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model = Transformer(len(chars), hidden_size, len(chars), num_layers, num_heads, dropout)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYa3wCpBWHRW",
        "outputId": "a6ca08b6-7f8c-4565-ad8c-60c6329f819c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_char(model, seq_len, char_to_int, int_to_char, initialstr):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    initial_input = torch.tensor([char_to_int[char] for char in initialstr], dtype=torch.long).unsqueeze(0).to(device)\n",
        "    output = model(initial_input)\n",
        "    _, predicted = torch.argmax(output, dim=1).item()\n",
        "    return int_to_char[predicted]"
      ],
      "metadata": {
        "id": "cF_7UT4rlY9H"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loop(train_loader, val_loader, model, criterion, optimizer, epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTC1MqMeWJmT",
        "outputId": "89ed7feb-15cc-4dd7-e588-b573fb096bb5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Train Loss: 322.1177, Val Loss: 318.0716, Validation Accuracy: 0.27%\n",
            "Epoch 2/10, Train Loss: 318.1293, Val Loss: 316.9726, Validation Accuracy: 0.27%\n",
            "Epoch 3/10, Train Loss: 317.4180, Val Loss: 316.3948, Validation Accuracy: 0.27%\n",
            "Epoch 4/10, Train Loss: 316.9148, Val Loss: 316.0687, Validation Accuracy: 0.27%\n",
            "Epoch 5/10, Train Loss: 316.8979, Val Loss: 316.3322, Validation Accuracy: 0.27%\n",
            "Epoch 6/10, Train Loss: 316.6735, Val Loss: 315.8450, Validation Accuracy: 0.27%\n",
            "Epoch 7/10, Train Loss: 316.2954, Val Loss: 315.7794, Validation Accuracy: 0.27%\n",
            "Epoch 8/10, Train Loss: 316.3851, Val Loss: 317.6847, Validation Accuracy: 0.27%\n",
            "Epoch 9/10, Train Loss: 316.1482, Val Loss: 315.3227, Validation Accuracy: 0.27%\n",
            "Epoch 10/10, Train Loss: 316.2447, Val Loss: 315.1842, Validation Accuracy: 0.27%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([322.1177186349036,\n",
              "  318.12928998398496,\n",
              "  317.4180045985177,\n",
              "  316.9148391453753,\n",
              "  316.89789083728283,\n",
              "  316.6734675831037,\n",
              "  316.29540742445596,\n",
              "  316.3850723575123,\n",
              "  316.1481586360617,\n",
              "  316.24474998800764],\n",
              " [318.07156016726776,\n",
              "  316.97261347294676,\n",
              "  316.39477219502265,\n",
              "  316.06867353600194,\n",
              "  316.3322217925417,\n",
              "  315.84503164718154,\n",
              "  315.7794421576797,\n",
              "  317.6847218078782,\n",
              "  315.32269242804284,\n",
              "  315.1842304296488])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sBbV16YLWMDp"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}